{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf885fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.utils import class_weight\n",
    "import warnings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau as lr_scheduler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd35a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config variables\n",
    "train_path = \"data/train\"\n",
    "dev_path = \"data/dev\"\n",
    "test_path = \"data/test\"\n",
    "glove_path = \"data/glove.6B.100d.gz\"\n",
    "unk = \"<unk>\"\n",
    "pad = \"<pad>\"\n",
    "num = \"<num>\"\n",
    "sym = \"<sym>\"\n",
    "max_len = 128\n",
    "batch_size = 8\n",
    "numbers = ['one','two','three','four','five', 'six','seven','eight','nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'zero', 'hundred', 'thousand', 'million', 'billion', 'trillion', 'quadrillion', 'quintillion', 'sextillion', 'septillion', 'octillion', 'nonillion', 'decillion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49aabd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        if \",\" in s:\n",
    "            s = s.replace(\",\", \"\")\n",
    "        if \":\" in s:\n",
    "            s = s.replace(\":\", \"\")\n",
    "        if \"-\" in s:\n",
    "            s = s.replace(\"-\", \"\")\n",
    "        if \"/\" in s:\n",
    "            s = s.replace(\"/\", \"\")\n",
    "        if \".\" in s:\n",
    "            s = s.replace(\".\", \"\")\n",
    "        if s.lower() in numbers:\n",
    "            return True\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "def is_symbol(s):\n",
    "    flag = 1\n",
    "    for char in s:\n",
    "        if char.isalnum():\n",
    "            flag = 0\n",
    "            break\n",
    "    if flag == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a4902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read the datafile in the \"path\" line by line and return a list of lists \n",
    "#after removing trailing white spaces and empty lists\n",
    "def read_data(path):\n",
    "    with open(path, \"r\") as file_obj:\n",
    "        lines = file_obj.readlines()\n",
    "    line_list = [line.rstrip().split() for line in lines]\n",
    "    line_list = [x for x in line_list if x != []]\n",
    "    return line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a96fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_data(train_path)\n",
    "dev_set = read_data(dev_path)\n",
    "test_set = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e91a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "tag_to_idx = {}\n",
    "idx_to_tag = {}\n",
    "just_tags = []\n",
    "idx = 0\n",
    "threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b06581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary from the training_data for task1,\n",
    "#removed the process of deleting words that are rare to increase performance\n",
    "#also, creating the tag to idx and idx to tag dictionaries\n",
    "for line in train_set:\n",
    "    word = line[1]\n",
    "    tag = line[2]\n",
    "    if is_number(word):\n",
    "        word = num\n",
    "    if word in vocab:\n",
    "        vocab[word] += 1\n",
    "    else:\n",
    "        vocab[word] = 1\n",
    "    if tag not in tag_to_idx:\n",
    "        tag_to_idx[tag] = idx\n",
    "        idx_to_tag[idx] = tag\n",
    "        idx += 1\n",
    "    just_tags.append(tag_to_idx[tag])\n",
    "# for word, freq in  list(vocab.items()):\n",
    "#     if freq <= threshold:\n",
    "#         del vocab[word]\n",
    "vocab[\"<unk>\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ec12be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set vocabulary : 20194\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of training set vocabulary :\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10383b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating word to idx and idx to word dictionaries to represent words as indices\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "idx = 1\n",
    "for word in vocab:\n",
    "    if word not in word_to_idx:\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "        idx += 1\n",
    "word_to_idx[pad] = 0\n",
    "idx_to_word[0] = pad\n",
    "pad_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea43769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return a numpy array of indices of the words padded to max_len = 128\n",
    "def dataset_creation(sentence, test = False):\n",
    "    global word_to_idx, tag_to_idx, unk, max_len\n",
    "    sen_len = len(sentence)\n",
    "    pad_len = max_len - sen_len\n",
    "    words = []\n",
    "    tags = []\n",
    "    if test:\n",
    "        for idx, word in sentence:\n",
    "            if word not in word_to_idx:\n",
    "                if is_number(word):\n",
    "                    word = num\n",
    "                else:\n",
    "                    word = unk\n",
    "            words.append(word_to_idx[word])\n",
    "        words = np.array(words)\n",
    "        pad_seq = pad_idx * np.ones(pad_len)\n",
    "        words = np.concatenate((words, pad_seq), axis = 0)\n",
    "        return words\n",
    "    else:\n",
    "        for idx, word, tag in sentence:\n",
    "            if word not in word_to_idx:\n",
    "                if is_number(word):\n",
    "                    word = num\n",
    "    #             elif word.isupper():\n",
    "    #                 temp = word[0] + word[1:].lower()\n",
    "    #                 if temp in word_to_idx:\n",
    "    #                     word = temp\n",
    "    #                 else:\n",
    "    #                     word = unk\n",
    "                else:\n",
    "                    word = unk\n",
    "            words.append(word_to_idx[word])\n",
    "            tags.append(tag_to_idx[tag])\n",
    "        words = np.array(words)\n",
    "        tags = np.array(tags)\n",
    "        pad_seq = pad_idx * np.ones(pad_len)\n",
    "        pad_tag = -1 * np.ones(pad_len)\n",
    "        words = np.concatenate((words, pad_seq), axis = 0)\n",
    "        tags = np.concatenate((tags, pad_tag), axis = 0)\n",
    "        return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5701c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the input and output data that is used to train the network\n",
    "#function calls dataset_creation on each sentence\n",
    "def create_lstm_ip(dataset, test = False):\n",
    "    x_lstm = []\n",
    "    y_lstm = []\n",
    "    sentence = []\n",
    "    for i in range(len(dataset)):\n",
    "        if i == len(dataset) - 1 or (dataset[i][0] == '1' and i != 0):\n",
    "            if test:\n",
    "                x = dataset_creation(sentence, test)\n",
    "                x_lstm.append(x)\n",
    "            else:\n",
    "                x, y = dataset_creation(sentence, test)\n",
    "                x_lstm.append(x)\n",
    "                y_lstm.append(y)\n",
    "            sentence = []\n",
    "            sentence.append(dataset[i])\n",
    "        else:\n",
    "            sentence.append(dataset[i])\n",
    "    if sentence != []:\n",
    "        if test:\n",
    "            last_x = dataset_creation(sentence, test)\n",
    "            x_lstm.append(last_x)\n",
    "        else:\n",
    "            last_x, last_y = dataset_creation(sentence)\n",
    "            x_lstm.append(last_x)\n",
    "            y_lstm.append(last_y)\n",
    "    if test:\n",
    "        return np.array(x_lstm)\n",
    "    else:\n",
    "        return np.array(x_lstm), np.array(y_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3672ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lstm_train, y_lstm_train = create_lstm_ip(train_set, False)\n",
    "x_lstm_dev, y_lstm_dev = create_lstm_ip(dev_set, False)\n",
    "x_lstm_test = create_lstm_ip(test_set, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db33c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset class definition\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "\n",
    "class data(Dataset):\n",
    "    def __init__(self, inputs, transform = None):\n",
    "        self.data = inputs\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.data[index][0]\n",
    "        label = self.data[index][1]\n",
    "        if self.transform is not None:\n",
    "            inputs = self.transform(inputs)\n",
    "            \n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca381173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertin the arrays into tensors\n",
    "x_lstm_train, x_lstm_dev, x_lstm_test = torch.LongTensor(x_lstm_train), torch.LongTensor(x_lstm_dev), torch.LongTensor(x_lstm_test)\n",
    "y_lstm_train, y_lstm_dev = torch.LongTensor(y_lstm_train), torch.LongTensor(y_lstm_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edb7dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return the original lengths of the padded sentences in a batch\n",
    "#function returns a tensor of batch_size containing the lengths of the corresponding sentence\n",
    "def get_lengths(seq, idx):\n",
    "    lens = []\n",
    "    for x in seq:\n",
    "        length = 0\n",
    "        for i in range(len(x)):\n",
    "            if x[i] == idx:\n",
    "                break\n",
    "            length += 1\n",
    "        lens.append(length)\n",
    "    return torch.Tensor(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "375a5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the data loaders\n",
    "lstm_train_dataset = TensorDataset(x_lstm_train, y_lstm_train)\n",
    "lstm_train_dataset = data(lstm_train_dataset)\n",
    "lstm_dev_dataset = TensorDataset(x_lstm_dev, y_lstm_dev)\n",
    "lstm_dev_dataset = data(lstm_dev_dataset)\n",
    "\n",
    "lstm_train_loader = DataLoader(lstm_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "lstm_dev_loader = DataLoader(lstm_dev_dataset, batch_size = batch_size, drop_last = True, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9236066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing network hyperparameters\n",
    "input_dim = len(word_to_idx)\n",
    "embed_dim = 100\n",
    "hidden_dim = 256\n",
    "linear_dim = 128\n",
    "output_dim = len(tag_to_idx)\n",
    "pad_idx = word_to_idx[pad]\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(just_tags), just_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04edeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy function to determine sentence level accuracy in a batch\n",
    "def accuracy(pred, targ):\n",
    "    pred = pred.argmax(dim = 1, keepdim = True)\n",
    "    non_pad_elements = (targ != -1).nonzero()\n",
    "    correct = pred[non_pad_elements].squeeze(1).eq(targ[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([targ[non_pad_elements].shape[0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90dadb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi-LSTM model with an embedding layer\n",
    "class bLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, linear_dim, output_dim, pad_idx):\n",
    "        super(bLSTM, self).__init__()\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings = input_dim, embedding_dim = embed_dim)\n",
    "        self.blstm = torch.nn.LSTM(input_size = embed_dim, hidden_size = hidden_dim, num_layers = 1, bidirectional = True, batch_first = True, dropout = 0.33)\n",
    "        self.linear = torch.nn.Linear(hidden_dim*2, linear_dim)\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.classifier = torch.nn.Linear(linear_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        lens = get_lengths(x, 0)\n",
    "        packed = pack_padded_sequence(emb, lens, batch_first = True, enforce_sorted = False)\n",
    "        blstm_out, _ = self.blstm(packed)\n",
    "        blstm_out, _ = pad_packed_sequence(blstm_out, batch_first = True, padding_value = 0, total_length = 128)\n",
    "        lin_out = self.elu(self.linear(blstm_out))\n",
    "        class_out = self.classifier(lin_out)\n",
    "        return class_out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            torch.nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "    def init_embeddings(self, padding_idx):\n",
    "        self.embedding.weight.data[padding_idx] = torch.zeros(self.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b67d4724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bLSTM(\n",
      "  (embedding): Embedding(20195, 100)\n",
      "  (blstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#initializing the model and the weights and printing the model architecture\n",
    "model = bLSTM(input_dim, embed_dim, hidden_dim, linear_dim, output_dim, pad_idx).to(device)\n",
    "model.init_weights()\n",
    "model.init_embeddings(padding_idx = pad_idx)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ce73958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the loss function, optimizer (with learning rate and momentum) and the scheduler (with step size)\n",
    "class_weights = torch.FloatTensor(class_weights)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights, ignore_index = -1).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.017, momentum = 0.9)\n",
    "scheduler = StepLR(optimizer, step_size = 25) \n",
    "# scheduler = lr_scheduler(optimizer, 'max', patience = 4, factor = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a952298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Acc: 0.580134 \tDev Set Acc: 0.699054\n",
      "Dev Set acc increased (0.000000 --> 0.699054). Saving model...\n",
      "Epoch: 2 \tTraining Acc: 0.806081 \tDev Set Acc: 0.787716\n",
      "Dev Set acc increased (0.699054 --> 0.787716). Saving model...\n",
      "Epoch: 3 \tTraining Acc: 0.893995 \tDev Set Acc: 0.865295\n",
      "Dev Set acc increased (0.787716 --> 0.865295). Saving model...\n",
      "Epoch: 4 \tTraining Acc: 0.932915 \tDev Set Acc: 0.928622\n",
      "Dev Set acc increased (0.865295 --> 0.928622). Saving model...\n",
      "Epoch: 5 \tTraining Acc: 0.955312 \tDev Set Acc: 0.942170\n",
      "Dev Set acc increased (0.928622 --> 0.942170). Saving model...\n",
      "Epoch: 6 \tTraining Acc: 0.969550 \tDev Set Acc: 0.948110\n",
      "Dev Set acc increased (0.942170 --> 0.948110). Saving model...\n",
      "Epoch: 7 \tTraining Acc: 0.978083 \tDev Set Acc: 0.936674\n",
      "Epoch: 8 \tTraining Acc: 0.981275 \tDev Set Acc: 0.952127\n",
      "Dev Set acc increased (0.948110 --> 0.952127). Saving model...\n",
      "Epoch: 9 \tTraining Acc: 0.983808 \tDev Set Acc: 0.944377\n",
      "Epoch: 10 \tTraining Acc: 0.988801 \tDev Set Acc: 0.957291\n",
      "Dev Set acc increased (0.952127 --> 0.957291). Saving model...\n",
      "Epoch: 11 \tTraining Acc: 0.990736 \tDev Set Acc: 0.955694\n",
      "Epoch: 12 \tTraining Acc: 0.993540 \tDev Set Acc: 0.957952\n",
      "Dev Set acc increased (0.957291 --> 0.957952). Saving model...\n",
      "Epoch: 13 \tTraining Acc: 0.994622 \tDev Set Acc: 0.958898\n",
      "Dev Set acc increased (0.957952 --> 0.958898). Saving model...\n",
      "Epoch: 14 \tTraining Acc: 0.995621 \tDev Set Acc: 0.949853\n",
      "Epoch: 15 \tTraining Acc: 0.996024 \tDev Set Acc: 0.959245\n",
      "Dev Set acc increased (0.958898 --> 0.959245). Saving model...\n",
      "Epoch: 16 \tTraining Acc: 0.996652 \tDev Set Acc: 0.959685\n",
      "Dev Set acc increased (0.959245 --> 0.959685). Saving model...\n",
      "Epoch: 17 \tTraining Acc: 0.996702 \tDev Set Acc: 0.958661\n",
      "Epoch: 18 \tTraining Acc: 0.997550 \tDev Set Acc: 0.959028\n",
      "Epoch: 19 \tTraining Acc: 0.998011 \tDev Set Acc: 0.958494\n",
      "Epoch: 20 \tTraining Acc: 0.998275 \tDev Set Acc: 0.957841\n",
      "Epoch: 21 \tTraining Acc: 0.997824 \tDev Set Acc: 0.957616\n",
      "Epoch: 22 \tTraining Acc: 0.998460 \tDev Set Acc: 0.958075\n",
      "Epoch: 23 \tTraining Acc: 0.998532 \tDev Set Acc: 0.959510\n",
      "Epoch: 24 \tTraining Acc: 0.998908 \tDev Set Acc: 0.958161\n",
      "Epoch: 25 \tTraining Acc: 0.998991 \tDev Set Acc: 0.958814\n",
      "Epoch: 26 \tTraining Acc: 0.999098 \tDev Set Acc: 0.958587\n",
      "Epoch: 27 \tTraining Acc: 0.999180 \tDev Set Acc: 0.957987\n",
      "Epoch: 28 \tTraining Acc: 0.998864 \tDev Set Acc: 0.957930\n",
      "Epoch: 29 \tTraining Acc: 0.999203 \tDev Set Acc: 0.957715\n",
      "Epoch: 30 \tTraining Acc: 0.999271 \tDev Set Acc: 0.959696\n",
      "Dev Set acc increased (0.959685 --> 0.959696). Saving model...\n",
      "Epoch: 31 \tTraining Acc: 0.999370 \tDev Set Acc: 0.959093\n",
      "Epoch: 32 \tTraining Acc: 0.999374 \tDev Set Acc: 0.958695\n",
      "Epoch: 33 \tTraining Acc: 0.999314 \tDev Set Acc: 0.958980\n",
      "Epoch: 34 \tTraining Acc: 0.999408 \tDev Set Acc: 0.958813\n",
      "Epoch: 35 \tTraining Acc: 0.999412 \tDev Set Acc: 0.958017\n",
      "Epoch: 36 \tTraining Acc: 0.999453 \tDev Set Acc: 0.959690\n",
      "Epoch: 37 \tTraining Acc: 0.999534 \tDev Set Acc: 0.958397\n",
      "Epoch: 38 \tTraining Acc: 0.999581 \tDev Set Acc: 0.958891\n",
      "Epoch: 39 \tTraining Acc: 0.999492 \tDev Set Acc: 0.958683\n",
      "Epoch: 40 \tTraining Acc: 0.999515 \tDev Set Acc: 0.958649\n",
      "Epoch: 41 \tTraining Acc: 0.999576 \tDev Set Acc: 0.958817\n",
      "Epoch: 42 \tTraining Acc: 0.999568 \tDev Set Acc: 0.958633\n",
      "Epoch: 43 \tTraining Acc: 0.999561 \tDev Set Acc: 0.957877\n",
      "Epoch: 44 \tTraining Acc: 0.999660 \tDev Set Acc: 0.958732\n",
      "Epoch: 45 \tTraining Acc: 0.999644 \tDev Set Acc: 0.958609\n",
      "Epoch: 46 \tTraining Acc: 0.999662 \tDev Set Acc: 0.958358\n",
      "Epoch: 47 \tTraining Acc: 0.999655 \tDev Set Acc: 0.959343\n",
      "Epoch: 48 \tTraining Acc: 0.999634 \tDev Set Acc: 0.958085\n",
      "Epoch: 49 \tTraining Acc: 0.999665 \tDev Set Acc: 0.958300\n",
      "Epoch: 50 \tTraining Acc: 0.999691 \tDev Set Acc: 0.958441\n",
      "Epoch: 51 \tTraining Acc: 0.999613 \tDev Set Acc: 0.958363\n",
      "Epoch: 52 \tTraining Acc: 0.999700 \tDev Set Acc: 0.958629\n",
      "Epoch: 53 \tTraining Acc: 0.999621 \tDev Set Acc: 0.957906\n",
      "Epoch: 54 \tTraining Acc: 0.999656 \tDev Set Acc: 0.957073\n",
      "Epoch: 55 \tTraining Acc: 0.999704 \tDev Set Acc: 0.958314\n",
      "Epoch: 56 \tTraining Acc: 0.999671 \tDev Set Acc: 0.958033\n",
      "Epoch: 57 \tTraining Acc: 0.999653 \tDev Set Acc: 0.958617\n",
      "Epoch: 58 \tTraining Acc: 0.999703 \tDev Set Acc: 0.958726\n",
      "Epoch: 59 \tTraining Acc: 0.999728 \tDev Set Acc: 0.958639\n",
      "Epoch: 60 \tTraining Acc: 0.999689 \tDev Set Acc: 0.957035\n",
      "Epoch: 61 \tTraining Acc: 0.999680 \tDev Set Acc: 0.958912\n",
      "Epoch: 62 \tTraining Acc: 0.999720 \tDev Set Acc: 0.958251\n",
      "Epoch: 63 \tTraining Acc: 0.999729 \tDev Set Acc: 0.957471\n",
      "Epoch: 64 \tTraining Acc: 0.999736 \tDev Set Acc: 0.958258\n",
      "Epoch: 65 \tTraining Acc: 0.999736 \tDev Set Acc: 0.958899\n",
      "Epoch: 66 \tTraining Acc: 0.999682 \tDev Set Acc: 0.958273\n",
      "Epoch: 67 \tTraining Acc: 0.999738 \tDev Set Acc: 0.958450\n",
      "Epoch: 68 \tTraining Acc: 0.999713 \tDev Set Acc: 0.958450\n",
      "Epoch: 69 \tTraining Acc: 0.999717 \tDev Set Acc: 0.958373\n",
      "Epoch: 70 \tTraining Acc: 0.999721 \tDev Set Acc: 0.959049\n",
      "Epoch: 71 \tTraining Acc: 0.999725 \tDev Set Acc: 0.959160\n",
      "Epoch: 72 \tTraining Acc: 0.999746 \tDev Set Acc: 0.958821\n",
      "Epoch: 73 \tTraining Acc: 0.999756 \tDev Set Acc: 0.958588\n",
      "Epoch: 74 \tTraining Acc: 0.999733 \tDev Set Acc: 0.958977\n",
      "Epoch: 75 \tTraining Acc: 0.999710 \tDev Set Acc: 0.958046\n",
      "Epoch: 76 \tTraining Acc: 0.999727 \tDev Set Acc: 0.957968\n",
      "Epoch: 77 \tTraining Acc: 0.999754 \tDev Set Acc: 0.958799\n",
      "Epoch: 78 \tTraining Acc: 0.999742 \tDev Set Acc: 0.958849\n",
      "Epoch: 79 \tTraining Acc: 0.999741 \tDev Set Acc: 0.957823\n",
      "Epoch: 80 \tTraining Acc: 0.999747 \tDev Set Acc: 0.959442\n",
      "Epoch: 81 \tTraining Acc: 0.999735 \tDev Set Acc: 0.958737\n",
      "Epoch: 82 \tTraining Acc: 0.999750 \tDev Set Acc: 0.957596\n",
      "Epoch: 83 \tTraining Acc: 0.999751 \tDev Set Acc: 0.958928\n",
      "Epoch: 84 \tTraining Acc: 0.999750 \tDev Set Acc: 0.957786\n",
      "Epoch: 85 \tTraining Acc: 0.999768 \tDev Set Acc: 0.958143\n",
      "Epoch: 86 \tTraining Acc: 0.999723 \tDev Set Acc: 0.958055\n",
      "Epoch: 87 \tTraining Acc: 0.999758 \tDev Set Acc: 0.958879\n",
      "Epoch: 88 \tTraining Acc: 0.999744 \tDev Set Acc: 0.958584\n",
      "Epoch: 89 \tTraining Acc: 0.999714 \tDev Set Acc: 0.957479\n",
      "Epoch: 90 \tTraining Acc: 0.999725 \tDev Set Acc: 0.958211\n",
      "Epoch: 91 \tTraining Acc: 0.999754 \tDev Set Acc: 0.958657\n",
      "Epoch: 92 \tTraining Acc: 0.999750 \tDev Set Acc: 0.958467\n",
      "Epoch: 93 \tTraining Acc: 0.999756 \tDev Set Acc: 0.958190\n",
      "Epoch: 94 \tTraining Acc: 0.999762 \tDev Set Acc: 0.958931\n",
      "Epoch: 95 \tTraining Acc: 0.999748 \tDev Set Acc: 0.958277\n",
      "Epoch: 96 \tTraining Acc: 0.999749 \tDev Set Acc: 0.957629\n",
      "Epoch: 97 \tTraining Acc: 0.999753 \tDev Set Acc: 0.957759\n",
      "Epoch: 98 \tTraining Acc: 0.999755 \tDev Set Acc: 0.958412\n",
      "Epoch: 99 \tTraining Acc: 0.999753 \tDev Set Acc: 0.958689\n",
      "Epoch: 100 \tTraining Acc: 0.999745 \tDev Set Acc: 0.957891\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "epochs = 100\n",
    "dev_max_acc = 0\n",
    "train_loader = lstm_train_loader\n",
    "dev_loader = lstm_dev_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_acc = 0.0\n",
    "    dev_acc = 0.0\n",
    "    batch = 0\n",
    "    for inputs, target in train_loader:\n",
    "        print(batch, end = \"\\r\")\n",
    "        batch += 1\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        target = target.view(-1)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += float(accuracy(output, target).item())\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in dev_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs, target = inputs, target\n",
    "        output = model(inputs)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        target = target.view(-1)\n",
    "        loss = loss_fn(output, target)\n",
    "        dev_acc += float(accuracy(output, target).item())\n",
    "    \n",
    "    train_acc = (train_acc * batch_size)/len(train_loader.dataset)\n",
    "    dev_acc = (dev_acc * batch_size)/len(dev_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Acc: {:.6f} \\tDev Set Acc: {:.6f}'.format(epoch+1, train_acc,dev_acc))\n",
    "    if dev_acc >= dev_max_acc:\n",
    "        print('Dev Set acc increased ({:.6f} --> {:.6f}). Saving model...'.format(dev_max_acc, dev_acc))\n",
    "        torch.save(model.state_dict(), 'model/blstm1.pt')\n",
    "        dev_max_acc = dev_acc\n",
    "    scheduler.step(dev_max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc82e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd61f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the output file in the required format\n",
    "def create_op_file(x, y, model, dataset, file, pad_idx, test = False):\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    line_num = 0\n",
    "    global idx_to_word, idx_to_tag\n",
    "    with torch.no_grad():\n",
    "        with open(file, \"w\") as fp:\n",
    "            for i in range(len(x)):\n",
    "                idx = 1\n",
    "                print(i, end = \"\\r\")\n",
    "                ip = x[i].to(device)\n",
    "                ip = torch.unsqueeze(ip, 0)\n",
    "                op = model(ip)\n",
    "                op = op.view(-1, op.shape[-1])\n",
    "                _, pred = torch.max(op, 1)\n",
    "                if test:\n",
    "                    for j in range(len(pred)):\n",
    "                        if x[i][j] == pad_idx:\n",
    "                            if i != len(x) - 1:\n",
    "                                fp.write(\"\\n\")\n",
    "                            break\n",
    "                        pred_tag = int(pred[j].item())\n",
    "                        z = dataset[line_num][1]\n",
    "                        fp.write(\"{} {} {}\\n\".format(idx, z, idx_to_tag[pred_tag]))\n",
    "                        line_num += 1\n",
    "                        idx += 1\n",
    "                else:\n",
    "                    target = y[i]\n",
    "                    for j in range(len(target)):\n",
    "                        if target[j] == -1:\n",
    "                            if i != len(x) - 1:\n",
    "                                fp.write(\"\\n\")\n",
    "                            break\n",
    "                        pred_tag = int(pred[j].item())\n",
    "                        targ_tag = int(target[j].item())\n",
    "                        if pred_tag == targ_tag:\n",
    "                            count += 1\n",
    "                        z = dataset[line_num][1]\n",
    "                        fp.write(\"{} {} {}\\n\".format(idx, z, idx_to_tag[pred_tag]))\n",
    "                        line_num += 1\n",
    "                        idx += 1\n",
    "#     print(count / line_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50a2e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bLSTM(\n",
      "  (embedding): Embedding(20195, 100)\n",
      "  (blstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n",
      "3465\r"
     ]
    }
   ],
   "source": [
    "#loading the model to predict on given dataset and store it as a file\n",
    "model = bLSTM(input_dim, embed_dim, hidden_dim, linear_dim, output_dim, pad_idx).to(device)\n",
    "print(model)\n",
    "model.load_state_dict(torch.load('model/blstm1.pt'))\n",
    "create_op_file(x_lstm_dev, y_lstm_dev, model, dev_set, \"outputs/dev1.out\", pad_idx, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2fa773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfcdd956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bLSTM(\n",
      "  (embedding): Embedding(20195, 100)\n",
      "  (blstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n",
      "3683\r"
     ]
    }
   ],
   "source": [
    "#loading the model to predict on given dataset and store it as a file\n",
    "model = bLSTM(input_dim, embed_dim, hidden_dim, linear_dim, output_dim, pad_idx).to(device)\n",
    "print(model)\n",
    "model.load_state_dict(torch.load('model/blstm1.pt'))\n",
    "create_op_file(x_lstm_test, None, model, test_set, \"outputs/test1.out\", pad_idx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fce5f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the list of lists which just contain the words, from the given dataset\n",
    "def create_corpus(dataset, test = False):\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    if test:\n",
    "        for idx, word in dataset:\n",
    "            if idx == '1' and sent != []:\n",
    "                sentences.append(sent)\n",
    "                sent = [word]\n",
    "            else:\n",
    "                sent.append(word)\n",
    "        if sent != []:\n",
    "            sentences.append(sent)\n",
    "    else:\n",
    "        for idx, word, tag in dataset:\n",
    "            if idx == '1' and sent != []:\n",
    "                sentences.append(sent)\n",
    "                sent = [word]\n",
    "            else:\n",
    "                sent.append(word)\n",
    "        if sent != []:\n",
    "            sentences.append(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0223b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a corpus that contains sentences from train, dev and test dataset\n",
    "train_corpus = create_corpus(train_set, False)\n",
    "dev_corpus = create_corpus(dev_set, False)\n",
    "test_corpus = create_corpus(test_set, True)\n",
    "corpus = train_corpus + dev_corpus + test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed3cafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the glove model\n",
    "glove_w2v_file = 'data/glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_path, glove_w2v_file)\n",
    "glove_vec = KeyedVectors.load_word2vec_format(glove_w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77adb324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create a global vocabulary consisting of words from train, dev and test sets\n",
    "#function creates the global word to idx and idx to word dictionaries\n",
    "def create_global_word2idx(corpus):\n",
    "    idx = 0\n",
    "    global all_word_to_idx, all_idx_to_word\n",
    "    for sent in corpus:\n",
    "        for word in sent:\n",
    "            if is_number(word):\n",
    "                word = num\n",
    "            if word not in all_word_to_idx:\n",
    "                all_word_to_idx[word] = idx\n",
    "                all_idx_to_word[idx] = word\n",
    "                idx += 1\n",
    "    all_word_to_idx[pad] = idx\n",
    "    all_idx_to_word[idx] = pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0dbe90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the weight matrix, for the embedding layer, from the glove vectors of each word in the corpus\n",
    "#words that do not have a glove embedding are assigned random embedding from a normal distribution\n",
    "#words are first converted into lower case and then assigned an embedding which is stored in a embedding dictionary\n",
    "#two words, capitalised and lower case are assigned the same embedding initially but they differ by their indices\n",
    "#since the embedding layer with the weight matrix is trainable, at the end of the training,\n",
    "#the embeddings of both those words should look different if they are not semantically similar to each other\n",
    "def create_weight_matrix(weight_matrix, model):\n",
    "    global all_word_to_idx, embedding_dict\n",
    "    for word, idx in all_word_to_idx.items():\n",
    "#         embed = np.zeros(100, dtype = float)\n",
    "        word = word.lower()\n",
    "        if word in embedding_dict:\n",
    "            weight_matrix[idx] = embedding_dict[word]\n",
    "        else:\n",
    "            try:\n",
    "                weight_matrix[idx] = model[word]\n",
    "            except KeyError:\n",
    "                rand_embed = np.random.normal(scale = 0.6, size = (100,))\n",
    "                weight_matrix[idx] = rand_embed\n",
    "                embedding_dict[word] = rand_embed\n",
    "    \n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a186aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pad the sentences\n",
    "def pad_glove(sentence):\n",
    "    global max_len, glove_pad\n",
    "    diff = max_len - len(sentence)\n",
    "    sentence = np.concatenate((sentence, glove_pad * np.ones(diff, dtype = float)))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cddc3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the dataset for bi-LSTM with GloVe embeddings\n",
    "def create_glove_data(sentences, test = False):\n",
    "    global all_word_to_idx\n",
    "    glove_sentences = []\n",
    "    glove_sent = []\n",
    "    if test:\n",
    "        for idx, word, in sentences:\n",
    "            if is_number(word):\n",
    "                word = num\n",
    "            if idx == '1' and glove_sent != []:\n",
    "                temp = np.array(glove_sent)\n",
    "                temp = pad_glove(temp)\n",
    "                glove_sentences.append(temp)\n",
    "                glove_sent = [all_word_to_idx[word]]\n",
    "            else:\n",
    "                glove_sent.append(all_word_to_idx[word])\n",
    "        if glove_sent != []:\n",
    "            temp = np.array(glove_sent)\n",
    "            temp = pad_glove(temp)\n",
    "            glove_sentences.append(temp)\n",
    "    else:\n",
    "        for idx, word, tag in sentences:\n",
    "            if is_number(word):\n",
    "                word = num\n",
    "            if idx == '1' and glove_sent != []:\n",
    "                temp = np.array(glove_sent)\n",
    "                temp = pad_glove(temp)\n",
    "                glove_sentences.append(temp)\n",
    "                glove_sent = [all_word_to_idx[word]]\n",
    "            else:\n",
    "                glove_sent.append(all_word_to_idx[word])\n",
    "        if glove_sent != []:\n",
    "            temp = np.array(glove_sent)\n",
    "            temp = pad_glove(temp)\n",
    "            glove_sentences.append(temp)\n",
    "    return np.array(glove_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe883cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the global corpus and dictionaries\n",
    "all_word_to_idx = {}\n",
    "all_idx_to_word = {}\n",
    "create_global_word2idx(corpus)\n",
    "glove_pad = all_word_to_idx[pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87bcecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the weight matrix\n",
    "weight_matrix = np.zeros((len(all_word_to_idx), 100), dtype = float)\n",
    "embedding_dict = {}\n",
    "embedding_dict[pad] = np.zeros(100, dtype = float)\n",
    "weight_matrix = create_weight_matrix(weight_matrix, glove_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f91b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = create_glove_data(train_set, False)\n",
    "dev_sent = create_glove_data(dev_set, False)\n",
    "test_sent = create_glove_data(test_set, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6dd222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_train_x, glove_dev_x, glove_test_x = torch.LongTensor(train_sent).to(device), torch.LongTensor(dev_sent).to(device), torch.LongTensor(test_sent).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db3c760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the tags and their indices are the same in both tasks, the tag tensors can be reused\n",
    "lstm_glove_train_dataset = TensorDataset(glove_train_x, y_lstm_train)\n",
    "lstm_glove_train_dataset = data(lstm_glove_train_dataset)\n",
    "lstm_glove_dev_dataset = TensorDataset(glove_dev_x, y_lstm_dev)\n",
    "lstm_glove_dev_dataset = data(lstm_glove_dev_dataset)\n",
    "\n",
    "lstm_glove_train_loader = DataLoader(lstm_glove_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "lstm_glove_dev_loader = DataLoader(lstm_glove_dev_dataset, batch_size = batch_size, drop_last = True, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c3cb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(all_word_to_idx)\n",
    "embed_dim = 100\n",
    "hidden_dim = 256\n",
    "linear_dim = 128\n",
    "output_dim = len(tag_to_idx)\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(just_tags), just_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d90ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the embedding layer and load the weights from the weight matrix\n",
    "#returns an embedding layer\n",
    "def create_embedding(input_dim, embed_dim, pad_idx, weight_matrix):\n",
    "    weight_matrix = torch.FloatTensor(weight_matrix).to(device)\n",
    "    embedding = torch.nn.Embedding(num_embeddings = input_dim, embedding_dim = embed_dim, padding_idx = pad_idx)\n",
    "    embedding.load_state_dict({'weight': weight_matrix})\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ae45dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi-LSTM with GloVe embeddings model\n",
    "#initialising weights of the network didn't improve performance\n",
    "class glove_bLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, linear_dim, output_dim, pad_idx, weight_matrix):\n",
    "        super(glove_bLSTM, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding = create_embedding(input_dim, embed_dim, pad_idx, weight_matrix)\n",
    "        self.blstm = torch.nn.LSTM(input_size = embed_dim, hidden_size = hidden_dim, num_layers = 1, bidirectional = True, batch_first = True, dropout = 0.33)\n",
    "        self.linear = torch.nn.Linear(hidden_dim * 2, linear_dim)\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.classifier = torch.nn.Linear(linear_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        pad_idx = self.pad_idx\n",
    "        lens = get_lengths(x, pad_idx)\n",
    "        packed = pack_padded_sequence(emb, lens, batch_first = True, enforce_sorted = False)\n",
    "        blstm_out, _ = self.blstm(packed)\n",
    "        blstm_out, _ = pad_packed_sequence(blstm_out, batch_first = True, padding_value = 0, total_length = 128)\n",
    "        lin_out = self.elu(self.linear(blstm_out))\n",
    "        class_out = self.classifier(lin_out)\n",
    "        return class_out\n",
    "    \n",
    "#     def init_weights(self):\n",
    "#         for name, param in self.named_parameters():\n",
    "#             torch.nn.init.normal_(param.data, mean=0, std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bb30ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_bLSTM(\n",
      "  (embedding): Embedding(25455, 100, padding_idx=25454)\n",
      "  (blstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = glove_bLSTM(input_dim, embed_dim, hidden_dim, linear_dim, output_dim, glove_pad, weight_matrix).to(device)\n",
    "# model.init_weights()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b335383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.FloatTensor(class_weights)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights, ignore_index = -1).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.017, momentum = 0.9)\n",
    "scheduler = StepLR(optimizer, step_size = 25) \n",
    "# scheduler = lr_scheduler(optimizer, 'max', patience = 4, factor = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4dd5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Acc: 0.760463 \tDev Set Acc: 0.609282\n",
      "Dev Set acc increased (0.000000 --> 0.609282). Saving model...\n",
      "Epoch: 2 \tTraining Acc: 0.852551 \tDev Set Acc: 0.890627\n",
      "Dev Set acc increased (0.609282 --> 0.890627). Saving model...\n",
      "Epoch: 3 \tTraining Acc: 0.889948 \tDev Set Acc: 0.915076\n",
      "Dev Set acc increased (0.890627 --> 0.915076). Saving model...\n",
      "Epoch: 4 \tTraining Acc: 0.909056 \tDev Set Acc: 0.913437\n",
      "Epoch: 5 \tTraining Acc: 0.926034 \tDev Set Acc: 0.940713\n",
      "Dev Set acc increased (0.915076 --> 0.940713). Saving model...\n",
      "Epoch: 6 \tTraining Acc: 0.940557 \tDev Set Acc: 0.943279\n",
      "Dev Set acc increased (0.940713 --> 0.943279). Saving model...\n",
      "Epoch: 7 \tTraining Acc: 0.949746 \tDev Set Acc: 0.947391\n",
      "Dev Set acc increased (0.943279 --> 0.947391). Saving model...\n",
      "Epoch: 8 \tTraining Acc: 0.958728 \tDev Set Acc: 0.949210\n",
      "Dev Set acc increased (0.947391 --> 0.949210). Saving model...\n",
      "Epoch: 9 \tTraining Acc: 0.965428 \tDev Set Acc: 0.957698\n",
      "Dev Set acc increased (0.949210 --> 0.957698). Saving model...\n",
      "Epoch: 10 \tTraining Acc: 0.971668 \tDev Set Acc: 0.956729\n",
      "Epoch: 11 \tTraining Acc: 0.976120 \tDev Set Acc: 0.946208\n",
      "Epoch: 12 \tTraining Acc: 0.979192 \tDev Set Acc: 0.964051\n",
      "Dev Set acc increased (0.957698 --> 0.964051). Saving model...\n",
      "Epoch: 13 \tTraining Acc: 0.981757 \tDev Set Acc: 0.969911\n",
      "Dev Set acc increased (0.964051 --> 0.969911). Saving model...\n",
      "Epoch: 14 \tTraining Acc: 0.984714 \tDev Set Acc: 0.965280\n",
      "Epoch: 15 \tTraining Acc: 0.986460 \tDev Set Acc: 0.967662\n",
      "Epoch: 16 \tTraining Acc: 0.989926 \tDev Set Acc: 0.971438\n",
      "Dev Set acc increased (0.969911 --> 0.971438). Saving model...\n",
      "Epoch: 17 \tTraining Acc: 0.991580 \tDev Set Acc: 0.972326\n",
      "Dev Set acc increased (0.971438 --> 0.972326). Saving model...\n",
      "Epoch: 18 \tTraining Acc: 0.992511 \tDev Set Acc: 0.974477\n",
      "Dev Set acc increased (0.972326 --> 0.974477). Saving model...\n",
      "Epoch: 19 \tTraining Acc: 0.993402 \tDev Set Acc: 0.974477\n",
      "Dev Set acc increased (0.974477 --> 0.974477). Saving model...\n",
      "Epoch: 20 \tTraining Acc: 0.994406 \tDev Set Acc: 0.975976\n",
      "Dev Set acc increased (0.974477 --> 0.975976). Saving model...\n",
      "Epoch: 21 \tTraining Acc: 0.994980 \tDev Set Acc: 0.974777\n",
      "Epoch: 22 \tTraining Acc: 0.995390 \tDev Set Acc: 0.974724\n",
      "Epoch: 23 \tTraining Acc: 0.995853 \tDev Set Acc: 0.974967\n",
      "Epoch: 24 \tTraining Acc: 0.996259 \tDev Set Acc: 0.976046\n",
      "Dev Set acc increased (0.975976 --> 0.976046). Saving model...\n",
      "Epoch: 25 \tTraining Acc: 0.996954 \tDev Set Acc: 0.976506\n",
      "Dev Set acc increased (0.976046 --> 0.976506). Saving model...\n",
      "Epoch: 26 \tTraining Acc: 0.997023 \tDev Set Acc: 0.976452\n",
      "Epoch: 27 \tTraining Acc: 0.997316 \tDev Set Acc: 0.976290\n",
      "Epoch: 28 \tTraining Acc: 0.997554 \tDev Set Acc: 0.976849\n",
      "Dev Set acc increased (0.976506 --> 0.976849). Saving model...\n",
      "Epoch: 29 \tTraining Acc: 0.997781 \tDev Set Acc: 0.976609\n",
      "Epoch: 30 \tTraining Acc: 0.998118 \tDev Set Acc: 0.976143\n",
      "Epoch: 31 \tTraining Acc: 0.998273 \tDev Set Acc: 0.976048\n",
      "Epoch: 32 \tTraining Acc: 0.998264 \tDev Set Acc: 0.976784\n",
      "Epoch: 33 \tTraining Acc: 0.998523 \tDev Set Acc: 0.976793\n",
      "Epoch: 34 \tTraining Acc: 0.998709 \tDev Set Acc: 0.977289\n",
      "Dev Set acc increased (0.976849 --> 0.977289). Saving model...\n",
      "Epoch: 35 \tTraining Acc: 0.998678 \tDev Set Acc: 0.976886\n",
      "Epoch: 36 \tTraining Acc: 0.998705 \tDev Set Acc: 0.977993\n",
      "Dev Set acc increased (0.977289 --> 0.977993). Saving model...\n",
      "Epoch: 37 \tTraining Acc: 0.998909 \tDev Set Acc: 0.977090\n",
      "Epoch: 38 \tTraining Acc: 0.998962 \tDev Set Acc: 0.977378\n",
      "Epoch: 39 \tTraining Acc: 0.998941 \tDev Set Acc: 0.978022\n",
      "Dev Set acc increased (0.977993 --> 0.978022). Saving model...\n",
      "Epoch: 40 \tTraining Acc: 0.999069 \tDev Set Acc: 0.977585\n",
      "Epoch: 41 \tTraining Acc: 0.999192 \tDev Set Acc: 0.976603\n",
      "Epoch: 42 \tTraining Acc: 0.999229 \tDev Set Acc: 0.977046\n",
      "Epoch: 43 \tTraining Acc: 0.999187 \tDev Set Acc: 0.977006\n",
      "Epoch: 44 \tTraining Acc: 0.999155 \tDev Set Acc: 0.977377\n",
      "Epoch: 45 \tTraining Acc: 0.999316 \tDev Set Acc: 0.977460\n",
      "Epoch: 46 \tTraining Acc: 0.999288 \tDev Set Acc: 0.977440\n",
      "Epoch: 47 \tTraining Acc: 0.999346 \tDev Set Acc: 0.977515\n",
      "Epoch: 48 \tTraining Acc: 0.999426 \tDev Set Acc: 0.977524\n",
      "Epoch: 49 \tTraining Acc: 0.999459 \tDev Set Acc: 0.977600\n",
      "Epoch: 50 \tTraining Acc: 0.999516 \tDev Set Acc: 0.977037\n",
      "Epoch: 51 \tTraining Acc: 0.999475 \tDev Set Acc: 0.977437\n",
      "Epoch: 52 \tTraining Acc: 0.999522 \tDev Set Acc: 0.977879\n",
      "Epoch: 53 \tTraining Acc: 0.999507 \tDev Set Acc: 0.977323\n",
      "Epoch: 54 \tTraining Acc: 0.999556 \tDev Set Acc: 0.977394\n",
      "Epoch: 55 \tTraining Acc: 0.999464 \tDev Set Acc: 0.977319\n",
      "Epoch: 56 \tTraining Acc: 0.999030 \tDev Set Acc: 0.976419\n",
      "Epoch: 57 \tTraining Acc: 0.992530 \tDev Set Acc: 0.974132\n",
      "Epoch: 58 \tTraining Acc: 0.984014 \tDev Set Acc: 0.960614\n",
      "Epoch: 59 \tTraining Acc: 0.988343 \tDev Set Acc: 0.974426\n",
      "Epoch: 60 \tTraining Acc: 0.993806 \tDev Set Acc: 0.972780\n",
      "Epoch: 61 \tTraining Acc: 0.996528 \tDev Set Acc: 0.975848\n",
      "Epoch: 62 \tTraining Acc: 0.998164 \tDev Set Acc: 0.977134\n",
      "Epoch: 63 \tTraining Acc: 0.998650 \tDev Set Acc: 0.977068\n",
      "Epoch: 64 \tTraining Acc: 0.998893 \tDev Set Acc: 0.977874\n",
      "Epoch: 65 \tTraining Acc: 0.999118 \tDev Set Acc: 0.977638\n",
      "Epoch: 66 \tTraining Acc: 0.999176 \tDev Set Acc: 0.977775\n",
      "Epoch: 67 \tTraining Acc: 0.999264 \tDev Set Acc: 0.978198\n",
      "Dev Set acc increased (0.978022 --> 0.978198). Saving model...\n",
      "Epoch: 68 \tTraining Acc: 0.999379 \tDev Set Acc: 0.977688\n",
      "Epoch: 69 \tTraining Acc: 0.999413 \tDev Set Acc: 0.977602\n",
      "Epoch: 70 \tTraining Acc: 0.999452 \tDev Set Acc: 0.977871\n",
      "Epoch: 71 \tTraining Acc: 0.999505 \tDev Set Acc: 0.977695\n",
      "Epoch: 72 \tTraining Acc: 0.999505 \tDev Set Acc: 0.977705\n",
      "Epoch: 73 \tTraining Acc: 0.999600 \tDev Set Acc: 0.977807\n",
      "Epoch: 74 \tTraining Acc: 0.999567 \tDev Set Acc: 0.976692\n",
      "Epoch: 75 \tTraining Acc: 0.999551 \tDev Set Acc: 0.978294\n",
      "Dev Set acc increased (0.978198 --> 0.978294). Saving model...\n",
      "Epoch: 76 \tTraining Acc: 0.999627 \tDev Set Acc: 0.977187\n",
      "Epoch: 77 \tTraining Acc: 0.999591 \tDev Set Acc: 0.977981\n",
      "Epoch: 78 \tTraining Acc: 0.999617 \tDev Set Acc: 0.977493\n",
      "Epoch: 79 \tTraining Acc: 0.999606 \tDev Set Acc: 0.978061\n",
      "Epoch: 80 \tTraining Acc: 0.999647 \tDev Set Acc: 0.977677\n",
      "Epoch: 81 \tTraining Acc: 0.999674 \tDev Set Acc: 0.977650\n",
      "Epoch: 82 \tTraining Acc: 0.999659 \tDev Set Acc: 0.977880\n",
      "Epoch: 83 \tTraining Acc: 0.999656 \tDev Set Acc: 0.978539\n",
      "Dev Set acc increased (0.978294 --> 0.978539). Saving model...\n",
      "Epoch: 84 \tTraining Acc: 0.999656 \tDev Set Acc: 0.977528\n",
      "Epoch: 85 \tTraining Acc: 0.999671 \tDev Set Acc: 0.977931\n",
      "Epoch: 86 \tTraining Acc: 0.999695 \tDev Set Acc: 0.977603\n",
      "Epoch: 87 \tTraining Acc: 0.999667 \tDev Set Acc: 0.978204\n",
      "Epoch: 88 \tTraining Acc: 0.999693 \tDev Set Acc: 0.977181\n",
      "Epoch: 89 \tTraining Acc: 0.999663 \tDev Set Acc: 0.977650\n",
      "Epoch: 90 \tTraining Acc: 0.999705 \tDev Set Acc: 0.978394\n",
      "Epoch: 91 \tTraining Acc: 0.999717 \tDev Set Acc: 0.978359\n",
      "Epoch: 92 \tTraining Acc: 0.999701 \tDev Set Acc: 0.978242\n",
      "Epoch: 93 \tTraining Acc: 0.999720 \tDev Set Acc: 0.977867\n",
      "Epoch: 94 \tTraining Acc: 0.999741 \tDev Set Acc: 0.977957\n",
      "Epoch: 95 \tTraining Acc: 0.999710 \tDev Set Acc: 0.978552\n",
      "Dev Set acc increased (0.978539 --> 0.978552). Saving model...\n",
      "Epoch: 96 \tTraining Acc: 0.999729 \tDev Set Acc: 0.978526\n",
      "Epoch: 97 \tTraining Acc: 0.999665 \tDev Set Acc: 0.978073\n",
      "Epoch: 98 \tTraining Acc: 0.999707 \tDev Set Acc: 0.977483\n",
      "Epoch: 99 \tTraining Acc: 0.999725 \tDev Set Acc: 0.978329\n",
      "Epoch: 100 \tTraining Acc: 0.999736 \tDev Set Acc: 0.978228\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "dev_max_acc = 0\n",
    "train_loader = lstm_glove_train_loader\n",
    "dev_loader = lstm_glove_dev_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_acc = 0.0\n",
    "    dev_acc = 0.0\n",
    "    batch = 0\n",
    "    for inputs, target in train_loader:\n",
    "        print(batch, end = \"\\r\")\n",
    "        batch += 1\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        target = target.view(-1)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += float(accuracy(output, target).item())\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in dev_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        inputs, target = inputs, target\n",
    "        output = model(inputs)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        target = target.view(-1)\n",
    "        loss = loss_fn(output, target)\n",
    "        dev_acc += float(accuracy(output, target).item())\n",
    "    \n",
    "    train_acc = (train_acc * batch_size)/len(train_loader.dataset)\n",
    "    dev_acc = (dev_acc * batch_size)/len(dev_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Acc: {:.6f} \\tDev Set Acc: {:.6f}'.format(epoch+1, train_acc,dev_acc))\n",
    "    if dev_acc >= dev_max_acc:\n",
    "        print('Dev Set acc increased ({:.6f} --> {:.6f}). Saving model...'.format(dev_max_acc, dev_acc))\n",
    "        torch.save(model.state_dict(), 'model/blstm2.pt')\n",
    "        dev_max_acc = dev_acc\n",
    "    scheduler.step(dev_max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53ac3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0e608a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3465\r"
     ]
    }
   ],
   "source": [
    "model = glove_bLSTM(input_dim, embed_dim, hidden_dim, linear_dim, output_dim, glove_pad, weight_matrix).to(device)\n",
    "model.load_state_dict(torch.load('blstm2.pt'))\n",
    "create_op_file(glove_dev_x, y_lstm_dev, model, dev_set, \"outputs/dev2.out\", glove_pad, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eecbdc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10f9bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3683\r"
     ]
    }
   ],
   "source": [
    "model = glove_bLSTM(input_dim, embed_dim, hidden_dim, linear_dim, output_dim, glove_pad, weight_matrix).to(device)\n",
    "model.load_state_dict(torch.load('blstm2.pt'))\n",
    "create_op_file(glove_test_x, None, model, test_set, \"outputs/test2.out\", glove_pad, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f9c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
